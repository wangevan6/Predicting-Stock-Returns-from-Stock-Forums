{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS1003 Final Project: Data Collectiing from Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticating with the Reddit API\n",
    "\n",
    "This code snippet demonstrates how to authenticate and interact with the Reddit API using OAuth.\n",
    "\n",
    "### Key Components of the Code:\n",
    "1. **Authentication Setup**: \n",
    "   - Utilizes the `requests` library to manage HTTP requests.\n",
    "   - The credentials, including `CLIENT_ID` (personal use script ID) and `SECRET_TOKEN` (OAuth token), are specified to perform HTTP basic authentication.\n",
    "\n",
    "2. **Requesting an OAuth Token**:\n",
    "   - The script sends a POST request containing the user credentials (username and password) to obtain an OAuth access token.\n",
    "   - This token is essential for authenticating subsequent API requests.\n",
    "\n",
    "3. **Setting Headers**:\n",
    "   - A `User-Agent` header provides Reddit with a brief description of the bot application.\n",
    "   - After obtaining the OAuth access token, it's added to the headers to enable authorization for further API requests.\n",
    "\n",
    "4. **Making Authenticated Requests**:\n",
    "   - Once the OAuth token is acquired, the `Authorization` header is included in all subsequent requests.\n",
    "   - In this example, an authenticated request fetches the profile information of the currently authenticated user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "executionInfo": {
     "elapsed": 1134,
     "status": "error",
     "timestamp": 1715196560624,
     "user": {
      "displayName": "Evan Wang",
      "userId": "09316293758748443763"
     },
     "user_tz": 240
    },
    "id": "pghcXAUkH9IT",
    "outputId": "ea5f6733-fbb8-4e87-a7cf-5e6e4523df45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "auth = requests.auth.HTTPBasicAuth('3gn07Oqp2_sut_zD3n8Dnw', 'WED8ZTRAfziqJPu-uugMfFGVhl-ozw')\n",
    "\n",
    "# here we pass our login method (password), username, and password\n",
    "data = {'grant_type': 'password',\n",
    "        'username': 'diamonddurfhands',\n",
    "        'password': '**DiamondHands22'}\n",
    "\n",
    "# setup our header info, which gives reddit a brief description of our app\n",
    "headers = {'User-Agent': 'MyBot/0.0.1'}\n",
    "\n",
    "# send our request for an OAuth token\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                    auth=auth, data=data, headers=headers)\n",
    "\n",
    "# convert response to JSON and pull access_token value\n",
    "TOKEN = res.json()['access_token']\n",
    "\n",
    "# add authorization to our headers dictionary\n",
    "headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "\n",
    "# while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving and Filtering Data from Reddit\n",
    "\n",
    "This section of code demonstrates how to use the OAuth token obtained earlier to retrieve and filter posts from a specific subreddit. The filtered data is then stored in a pandas DataFrame for further analysis.\n",
    "\n",
    "### Key Components of the Code:\n",
    "1. **Query Parameters**:\n",
    "   - The `params` dictionary specifies the number of posts to retrieve (`limit: 1000`).\n",
    "\n",
    "2. **Making a GET Request**:\n",
    "   - The GET request retrieves submissions that meet the specified search criteria:\n",
    "     - Subreddit: `wallstreetbets`\n",
    "     - Flair: `DD` (Deep Dive)\n",
    "     - Sorted by newest posts.\n",
    "\n",
    "3. **Initializing an Empty DataFrame**:\n",
    "   - A pandas DataFrame is initialized to store the relevant fields from the posts.\n",
    "\n",
    "4. **Filtering and Storing Posts**:\n",
    "   - Each post retrieved is checked for the desired flair and subreddit name.\n",
    "   - If it matches the criteria, a new DataFrame is created from that post's data and concatenated with the main DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tCRFNpyTJlXa"
   },
   "outputs": [],
   "source": [
    "params = {'limit': 1000}\n",
    "res = requests.get(\"https://oauth.reddit.com/r/wallstreetbets/search/?q=flair%3ADD&restrict_sr=on&include_over_18=on&sort=new\",\n",
    "                   headers=headers,\n",
    "                   params=params)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()  # initialize dataframe\n",
    "\n",
    "# loop through each post retrieved from GET request\n",
    "for post in res.json()['data']['children']:\n",
    "\n",
    "  # append relevant data to dataframe\n",
    "  if post['data']['link_flair_richtext'] == [{'e': 'text', 't': 'DD'}] and post['data']['subreddit'] == \"wallstreetbets\":\n",
    "    new_data = pd.DataFrame([{\n",
    "        'subreddit': post['data']['subreddit'],\n",
    "        'title': post['data']['title'],\n",
    "        'selftext': post['data']['selftext'],\n",
    "        'upvote_ratio': post['data']['upvote_ratio'],\n",
    "        'ups': post['data']['ups'],\n",
    "        'downs': post['data']['downs'],\n",
    "        'score': post['data']['score'],\n",
    "        'flair': post['data']['link_flair_richtext']\n",
    "    }])\n",
    "      # Concatenate with the existing DataFrame\n",
    "    df = pd.concat([df, new_data], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1664921984643,
     "user": {
      "displayName": "Sunny Yang",
      "userId": "14334615308793811067"
     },
     "user_tz": 240
    },
    "id": "EXA3bIa6J4TS",
    "outputId": "0fb2c1d6-ab1d-4236-e241-85fa841f4100"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>score</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>PG&amp;amp;E California Power Utility ($PCG)</td>\n",
       "      <td>Richmond, California just passed a resolution ...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>[{'e': 'text', 't': 'DD'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Uber position update and thoughts going forward</td>\n",
       "      <td>**Position Update**\\n\\nAbout a month ago I pos...</td>\n",
       "      <td>0.73</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>[{'e': 'text', 't': 'DD'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>INTC Outlook and Marketcap vs Assets</td>\n",
       "      <td>Why is foundry for Intel so important, despite...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>[{'e': 'text', 't': 'DD'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>DD - Put ProKidney on your radar</td>\n",
       "      <td>I know y'all like shouty AI-generated DD full ...</td>\n",
       "      <td>0.76</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>[{'e': 'text', 't': 'DD'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Robinhood Q1 Earnings - Is It A Trap?</td>\n",
       "      <td>**Summary:**\\n\\nI believe Robinhood is poised ...</td>\n",
       "      <td>0.80</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>[{'e': 'text', 't': 'DD'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Cannabis - not too late to get high bros</td>\n",
       "      <td>EDIT:  I WAS RIGHT YOU REGARDS!!! LETS FUCKIN ...</td>\n",
       "      <td>0.84</td>\n",
       "      <td>4571</td>\n",
       "      <td>0</td>\n",
       "      <td>4571</td>\n",
       "      <td>[{'e': 'text', 't': 'DD'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>CHK-SWN merger to create largest US natural ga...</td>\n",
       "      <td>I’ve been following Chesapeake Energy since Ja...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>[{'e': 'text', 't': 'DD'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Solventum- Potentially undervalued spin-off</td>\n",
       "      <td>The following Due Diligence is pretty simple a...</td>\n",
       "      <td>0.72</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>[{'e': 'text', 't': 'DD'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>TSMC Earthquake DD</td>\n",
       "      <td>**Like most of you guys I was initially worrie...</td>\n",
       "      <td>0.81</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>[{'e': 'text', 't': 'DD'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>$INTC as serious AI inference play</td>\n",
       "      <td>I keep reading posts about Intel and consumer ...</td>\n",
       "      <td>0.71</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>[{'e': 'text', 't': 'DD'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         subreddit                                              title  \\\n",
       "0   wallstreetbets           PG&amp;E California Power Utility ($PCG)   \n",
       "1   wallstreetbets    Uber position update and thoughts going forward   \n",
       "2   wallstreetbets               INTC Outlook and Marketcap vs Assets   \n",
       "3   wallstreetbets                   DD - Put ProKidney on your radar   \n",
       "4   wallstreetbets              Robinhood Q1 Earnings - Is It A Trap?   \n",
       "..             ...                                                ...   \n",
       "94  wallstreetbets           Cannabis - not too late to get high bros   \n",
       "95  wallstreetbets  CHK-SWN merger to create largest US natural ga...   \n",
       "96  wallstreetbets        Solventum- Potentially undervalued spin-off   \n",
       "97  wallstreetbets                                 TSMC Earthquake DD   \n",
       "98  wallstreetbets                 $INTC as serious AI inference play   \n",
       "\n",
       "                                             selftext  upvote_ratio   ups  \\\n",
       "0   Richmond, California just passed a resolution ...          0.91     9   \n",
       "1   **Position Update**\\n\\nAbout a month ago I pos...          0.73     9   \n",
       "2   Why is foundry for Intel so important, despite...          0.70     9   \n",
       "3   I know y'all like shouty AI-generated DD full ...          0.76     9   \n",
       "4   **Summary:**\\n\\nI believe Robinhood is poised ...          0.80    32   \n",
       "..                                                ...           ...   ...   \n",
       "94  EDIT:  I WAS RIGHT YOU REGARDS!!! LETS FUCKIN ...          0.84  4571   \n",
       "95  I’ve been following Chesapeake Energy since Ja...          0.92    70   \n",
       "96  The following Due Diligence is pretty simple a...          0.72    13   \n",
       "97  **Like most of you guys I was initially worrie...          0.81    71   \n",
       "98  I keep reading posts about Intel and consumer ...          0.71    39   \n",
       "\n",
       "    downs  score                       flair  \n",
       "0       0      9  [{'e': 'text', 't': 'DD'}]  \n",
       "1       0      9  [{'e': 'text', 't': 'DD'}]  \n",
       "2       0      9  [{'e': 'text', 't': 'DD'}]  \n",
       "3       0      9  [{'e': 'text', 't': 'DD'}]  \n",
       "4       0     32  [{'e': 'text', 't': 'DD'}]  \n",
       "..    ...    ...                         ...  \n",
       "94      0   4571  [{'e': 'text', 't': 'DD'}]  \n",
       "95      0     70  [{'e': 'text', 't': 'DD'}]  \n",
       "96      0     13  [{'e': 'text', 't': 'DD'}]  \n",
       "97      0     71  [{'e': 'text', 't': 'DD'}]  \n",
       "98      0     39  [{'e': 'text', 't': 'DD'}]  \n",
       "\n",
       "[99 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here is a brif overview of the data collected\n",
    "df\n",
    "df.to_csv('wsbdata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pushshift API to Retrieve Reddit Submissions\n",
    "\n",
    "In this section, we access Reddit submissions via the Pushshift API. The collected data is organized into a pandas DataFrame for further analysis.\n",
    "\n",
    "### Key Components of the Code:\n",
    "1. **Pushshift API Request Function** (`getPushshiftData`):\n",
    "   - This function constructs a query URL based on parameters like title (`query`), time frame (`after`, `before`), and subreddit name (`sub`).\n",
    "   - The request is sent using the `requests` library, and the JSON response is parsed to obtain the submissions' data.\n",
    "\n",
    "2. **Data Extraction Function** (`collectSubData`):\n",
    "   - The function extracts key data points (title, URL, author, score, etc.) from each submission.\n",
    "   - Error handling ensures that missing flair and selftext fields are marked as \"NaN.\"\n",
    "   - The extracted data is organized into a pandas DataFrame.\n",
    "\n",
    "3. **Concatenating Submissions**:\n",
    "   - The submission data for each post is merged into a global DataFrame (`subStats_df`) using `pd.concat`.\n",
    "   - The global DataFrame is updated within the function to accumulate all the submissions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Va5lJXRSIABX"
   },
   "outputs": [],
   "source": [
    "# going to try pushshift api\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def getPushshiftData(query, after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "#This function will be used to extract the key data points from each JSON result\n",
    "def collectSubData(subm):\n",
    "    try:\n",
    "        flair = subm['link_flair_richtext']\n",
    "    except KeyError:\n",
    "        flair = \"NaN\"  # Handle missing flair\n",
    "    try:\n",
    "        selftext = subm['selftext']\n",
    "    except KeyError:\n",
    "        selftext = \"NaN\"  # Handle missing selftext\n",
    "\n",
    "    # Create a DataFrame for the single submission\n",
    "    subData = pd.DataFrame([{\n",
    "        'sub_id': subm['id'],\n",
    "        'title': subm['title'],\n",
    "        'url': subm['url'],\n",
    "        'author': subm['author'],\n",
    "        'score': subm['score'],\n",
    "        'created': datetime.datetime.fromtimestamp(subm['created_utc']),\n",
    "        'selftext': selftext,\n",
    "        'flair': flair\n",
    "    }])\n",
    "    # Use pd.concat to merge with the global DataFrame\n",
    "    global subStats_df  # To modify the global DataFrame within the function\n",
    "    subStats_df = pd.concat([subStats_df, new_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Search Parameters for the Pushshift API\n",
    "\n",
    "In this section, the code sets up essential search parameters for querying submissions from the Pushshift API.\n",
    "\n",
    "### Key Components of the Code:\n",
    "1. **Time Range**:\n",
    "   - The `after` and `before` variables represent Unix timestamps that define the time range for the search.\n",
    "   - Unix timestamps can be created or converted using online tools like [unixtimestamp.com](https://www.unixtimestamp.com/index.php).\n",
    "\n",
    "2. **Search Query**:\n",
    "   - The `query` variable specifies keywords to search for within submissions (e.g., \"DD\" for Deep Dive).\n",
    "\n",
    "3. **Subreddit**:\n",
    "   - The `sub` variable defines the specific subreddit to query, in this case, `wallstreetbets`.\n",
    "\n",
    "4. **Tracking Data Collection**:\n",
    "   - `subCount`: Tracks the total number of submissions collected.\n",
    "   - `subStats`: A dictionary used to store collected submissions data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d7aRSoI9IrH1"
   },
   "outputs": [],
   "source": [
    "#Create your timestamps and queries for your search URL\n",
    "#https://www.unixtimestamp.com/index.php > Use this to create your timestamps\n",
    "\n",
    "after = \"1672549200\" #Submissions after this timestamp\n",
    "before = \"1641013200\" #Submissions before this timestamp\n",
    "query = \"DD\" #Keyword(s) to look for in submissions\n",
    "sub = \"wallstreetbets\" #Which Subreddit to search in\n",
    "\n",
    "#subCount tracks the no. of total submissions we collect\n",
    "subCount = 0\n",
    "#subStats is the dictionary where we will store our data.\n",
    "subStats = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteratively Collecting Reddit Submissions\n",
    "\n",
    "In this section, we utilize the functions defined earlier to iteratively collect submissions using the Pushshift API, while adjusting the search range dynamically.\n",
    "\n",
    "### Key Components of the Code:\n",
    "1. **Initial API Call**:\n",
    "   - The function `getPushshiftData` is called first with the given parameters (`after`, `before`, `query`, and `sub`) to initialize the `data` variable.\n",
    "\n",
    "2. **While Loop for Data Collection**:\n",
    "   - The loop continues until `data` is empty, indicating no more submissions in the specified range.\n",
    "   - Inside the loop:\n",
    "     - **Data Extraction**: Each submission in the `data` list is passed to `collectSubData` for processing and storage, and the submission count (`subCount`) is incremented.\n",
    "     - **Updating the 'After' Variable**: The `after` variable is updated to the `created_utc` timestamp of the last submission retrieved. This ensures that the next API call continues where the previous left off.\n",
    "\n",
    "3. **Fetching More Data**:\n",
    "   - The API call is repeated using the updated `after` value, collecting more data within the specified range until no new data is found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1123,
     "status": "ok",
     "timestamp": 1674619397671,
     "user": {
      "displayName": "Sunny Yang",
      "userId": "14334615308793811067"
     },
     "user_tz": 300
    },
    "id": "Wx8bCdIOItcg",
    "outputId": "1fcdfb5a-76b4-4db5-8995-c7fc32ef5ccd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?title=DD&size=1000&after=1672549200&before=1641013200&subreddit=wallstreetbets\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# We need to run this function outside the loop first to get the updated after variable\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m getPushshiftData(query, after, before, sub)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Will run until all posts have been gathered i.e. When the length of data variable = 0\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# from the 'after' date up until before date\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m#The length of data is the number submissions (data[0], data[1] etc), once it hits zero (after and before vars are the same) end\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m, in \u001b[0;36mgetPushshiftData\u001b[0;34m(query, after, before, sub)\u001b[0m\n\u001b[1;32m     12\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(r\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "# We need to run this function outside the loop first to get the updated after variable\n",
    "data = getPushshiftData(query, after, before, sub)\n",
    "# Will run until all posts have been gathered i.e. When the length of data variable = 0\n",
    "# from the 'after' date up until before date\n",
    "while len(data) > 0: #The length of data is the number submissions (data[0], data[1] etc), once it hits zero (after and before vars are the same) end\n",
    "    for submission in data:\n",
    "        collectSubData(submission)\n",
    "        subCount+=1\n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    print(len(data))\n",
    "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    #update after variable to last created date of submission\n",
    "    after = data[-1]['created_utc']\n",
    "    #data has changed due to the new after variable provided by above code\n",
    "    data = getPushshiftData(query, after, before, sub)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1674619342299,
     "user": {
      "displayName": "Sunny Yang",
      "userId": "14334615308793811067"
     },
     "user_tz": 300
    },
    "id": "GVSTN7e5Iu2b",
    "outputId": "9f7ca6c8-e1f7-4083-9e60-6434e1779fd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171 submissions have added to list\n",
      "1st entry is:\n",
      "watch list for Monday 23rd...do your own DD no financial advice created: 2023-01-22 22:23:03\n",
      "Last entry is:\n",
      "NAIL in the Coffin (light dd) created: 2022-11-04 16:35:10\n"
     ]
    }
   ],
   "source": [
    "print(str(len(subStats)) + \" submissions have added to list\")\n",
    "print(\"1st entry is:\")\n",
    "print(list(subStats.values())[0][0][1] + \" created: \" + str(list(subStats.values())[0][0][5]))\n",
    "print(\"Last entry is:\")\n",
    "print(list(subStats.values())[-1][0][1] + \" created: \" + str(list(subStats.values())[-1][0][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12649,
     "status": "ok",
     "timestamp": 1674619360778,
     "user": {
      "displayName": "Sunny Yang",
      "userId": "14334615308793811067"
     },
     "user_tz": 300
    },
    "id": "MBikywNJ8ufl",
    "outputId": "127740df-6de8-41e2-cca6-86bc83540e68",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsbpushdata.csv\n",
      "2023datafirsttest.csv\n",
      "171 submissions have been uploaded\n"
     ]
    }
   ],
   "source": [
    "def updateSubs_file():\n",
    "    upload_count = 0\n",
    "    #location = \"\\\\Reddit Data\\\\\" >> If you're running this outside of a notebook you'll need this to direct to a specific location\n",
    "    print(\"wsbpushdata.csv\")\n",
    "    filename = input() #This asks the user what to name the file\n",
    "    file = filename\n",
    "    with open(file, 'w', newline='', encoding='utf-8') as file:\n",
    "        a = csv.writer(file, delimiter=',')\n",
    "        headers = [\"Post ID\",\"Title\",\"Url\",\"Author\",\"Score\",\"Publish Date\", \"Selftext\", \"Flair\"]\n",
    "        a.writerow(headers)\n",
    "        for sub in subStats:\n",
    "            a.writerow(subStats[sub][0])\n",
    "            upload_count+=1\n",
    "\n",
    "        print(str(upload_count) + \" submissions have been uploaded\")\n",
    "updateSubs_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing `reddit-data-collector` for Efficient Data Collection\n",
    "\n",
    "To simplify Reddit data collection, we utilize the `reddit-data-collector` Python package. Installing it with `pip` helps us manage Reddit API queries more efficiently.\n",
    "\n",
    "### Key Benefits and Usage:\n",
    "1. **Effortless API Management**:\n",
    "   - The package abstracts complexities of Reddit's API, allowing faster data collection for different types of Reddit posts and comments.\n",
    "   - This aligns well with our goal of fetching a significant amount of posts using Pushshift or the official Reddit API.\n",
    "\n",
    "2. **Incorporating with Existing Code**:\n",
    "   - Using `reddit-data-collector` can replace custom API calls and data processing logic.\n",
    "   - It provides pre-built functions to specify search criteria, similar to `getPushshiftData`.\n",
    "\n",
    "3. **Installation Command**:\n",
    "   - To install the package, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6716,
     "status": "ok",
     "timestamp": 1674619784354,
     "user": {
      "displayName": "Sunny Yang",
      "userId": "14334615308793811067"
     },
     "user_tz": 300
    },
    "id": "URaTwZssHTb8",
    "outputId": "c82812e0-5500-4037-c61d-5d4e1e8d54f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting reddit-data-collector\n",
      "  Obtaining dependency information for reddit-data-collector from https://files.pythonhosted.org/packages/62/9d/3994784d7609692163b50a2682dbaaaf8a00387b287a515267af1d9f6d6b/reddit_data_collector-1.1.0-py3-none-any.whl.metadata\n",
      "  Downloading reddit_data_collector-1.1.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: pandas>=1.3.5 in /Users/water/anaconda3/lib/python3.11/site-packages (from reddit-data-collector) (2.1.4)\n",
      "Collecting praw>=7.5.0 (from reddit-data-collector)\n",
      "  Obtaining dependency information for praw>=7.5.0 from https://files.pythonhosted.org/packages/81/6a/21bc058bcccbe03f6a0895bf1bd60c805f0c526aa4e9bfaac775ed0b299c/praw-7.7.1-py3-none-any.whl.metadata\n",
      "  Downloading praw-7.7.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in /Users/water/anaconda3/lib/python3.11/site-packages (from reddit-data-collector) (4.65.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /Users/water/anaconda3/lib/python3.11/site-packages (from pandas>=1.3.5->reddit-data-collector) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/water/anaconda3/lib/python3.11/site-packages (from pandas>=1.3.5->reddit-data-collector) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/water/anaconda3/lib/python3.11/site-packages (from pandas>=1.3.5->reddit-data-collector) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/water/anaconda3/lib/python3.11/site-packages (from pandas>=1.3.5->reddit-data-collector) (2023.3)\n",
      "Collecting prawcore<3,>=2.1 (from praw>=7.5.0->reddit-data-collector)\n",
      "  Obtaining dependency information for prawcore<3,>=2.1 from https://files.pythonhosted.org/packages/96/5c/8af904314e42d5401afcfaff69940dc448e974f80f7aa39b241a4fbf0cf1/prawcore-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update-checker>=0.18 (from praw>=7.5.0->reddit-data-collector)\n",
      "  Obtaining dependency information for update-checker>=0.18 from https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Users/water/anaconda3/lib/python3.11/site-packages (from praw>=7.5.0->reddit-data-collector) (0.58.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /Users/water/anaconda3/lib/python3.11/site-packages (from prawcore<3,>=2.1->praw>=7.5.0->reddit-data-collector) (2.31.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/water/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->reddit-data-collector) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/water/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw>=7.5.0->reddit-data-collector) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/water/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw>=7.5.0->reddit-data-collector) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/water/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw>=7.5.0->reddit-data-collector) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/water/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw>=7.5.0->reddit-data-collector) (2024.2.2)\n",
      "Downloading reddit_data_collector-1.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: update-checker, prawcore, praw, reddit-data-collector\n",
      "Successfully installed praw-7.7.1 prawcore-2.4.0 reddit-data-collector-1.1.0 update-checker-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install reddit-data-collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Reddit API Credentials\n",
    "\n",
    "Here, we store sensitive Reddit API credentials and user details as variables to authenticate requests to Reddit's API.\n",
    "\n",
    "### Components Explained:\n",
    "\n",
    "1. **Credentials**:\n",
    "   - `client_id`: Unique identifier assigned to the application via Reddit's developer dashboard.\n",
    "   - `client_secret`: Secret token used alongside `client_id` for secure API access.\n",
    "   - `user_agent`: Custom identifier string that describes the application. This should include a name and version, like `\"myapp/0.1\"`.\n",
    "\n",
    "2. **User Credentials**:\n",
    "   - `username`: Reddit account username, necessary for authenticated access to private data or posting.\n",
    "   - `password`: Corresponding password to authenticate the Reddit account.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "R4uAGeb2HqyQ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "client_id = \"3gn07Oqp2_sut_zD3n8Dnw\"\n",
    "client_secret = \"WED8ZTRAfziqJPu-uugMfFGVhl-ozw\"\n",
    "user_agent = \"ddhandsbot\"\n",
    "username = \"diamonddurfhands\"\n",
    "password = \"**DiamondHands22\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Reddit Data Collection\n",
    "\n",
    "This section describes the data collection process using a Reddit Data Collector class. The purpose is to gather structured data about posts and comments from specified subreddits on Reddit.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **`DataCollector` Class**  \n",
    "   - **Initialization**: Requires Reddit API credentials (client ID, secret, and user agent) and optionally the username and password to access the Reddit API.\n",
    "   - **Subreddit Verification**: The class checks if specified subreddits exist before attempting to collect data, raising a `SubredditError` if not.\n",
    "   - **Post Filters**: Validates the filtering method for posts (e.g., \"hot\", \"new\", \"top\") and raises a `FilterError` if an invalid filter is used.\n",
    "   - **Top Post Filters**: Similar validation ensures the \"top\" filter only uses valid parameters (like \"day,\" \"week,\" etc.).\n",
    "\n",
    "2. **Data Collection Methods**  \n",
    "   - **Post Data**: Retrieves posts based on the specified filter method (e.g., \"new,\" \"hot,\" \"top\"). Each post's attributes include title, score, URL, and other metadata.\n",
    "   - **Comment Data**: Optionally gathers comments and replies to each post if enabled. The depth of data collected can be configured via parameters.\n",
    "   - **Output Format**: Data can be returned as either pandas DataFrames or Python dictionaries for posts and comments.\n",
    "\n",
    "3. **Helper Functions**  \n",
    "   - Subreddit verification and filter validation functions assist with error handling.\n",
    "   - Post and comment retrieval functions are modular, streamlining data collection based on various criteria.\n",
    "\n",
    "The class ensures that users can efficiently collect data from Reddit by handling errors gracefully, maximizing API usage, and providing flexibility in data organization and structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rY4LONQ0KEp9"
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "def to_pandas(subreddit_data, separate=False):\n",
    "    \"\"\"Convert raw post or comment data collected to a pandas `DataFrame`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    subreddit_data : dict\n",
    "        Raw post or comment data collected with the `DataCollector.get_data`\n",
    "        method.\n",
    "    separate : bool, default=False\n",
    "        Whether or not to return a separate pandas `DataFrame` for the\n",
    "        data of each subreddit.\n",
    "    Returns\n",
    "    -------\n",
    "    df or dfs : pd.DataFrame or dict\n",
    "        If separate is `False`, returns a pandas `DataFrame` containing\n",
    "        the post or comment data.\n",
    "        If separate is `True` returns a Python dictionary containing\n",
    "        a pandas `DataFrame` for each subreddit that existed in the\n",
    "        post or comment data.  The dctionary keys are the subreddits\n",
    "        names.  The dictionary values are pandas `DataFrame`s of post\n",
    "        or comment data.\n",
    "    See Also\n",
    "    --------\n",
    "    reddit_data_collector.reddit_data_collector.DataCollector\n",
    "        Class that performs the data collection from Reddit.\n",
    "    reddit_data_collector.io.update_data\n",
    "        Update a `.csv` file containing existing post or comment\n",
    "        data collected with new data collected with `DataCollector`.\n",
    "    \"\"\"\n",
    "    dfs = dict()\n",
    "\n",
    "    for subreddit, data in subreddit_data.items():\n",
    "        dfs[subreddit] = pd.DataFrame(data)\n",
    "\n",
    "    if separate:\n",
    "        return dfs\n",
    "    else:\n",
    "        return pd.concat(dfs.values(), ignore_index=True)\n",
    "\n",
    "\n",
    "def update_data(csv_path, df, key=\"id\", sort=\"subreddit_name\", save=True):\n",
    "    \"\"\"Update a `.csv` file containing post or comment data with new data.\n",
    "    The main purpose of this method is to allow a user to update a `.csv`\n",
    "    file that contains historical data that they collected with Reddit Data\n",
    "    Collector with new data collected.  The default method settings ensure\n",
    "    that duplicated post or comment data, if any, is not saved to the `.csv`\n",
    "    file.  In other words only one copy of each post or comment is kept in\n",
    "    the combined data.\n",
    "    If the `save` parameter is set to `True` then the method will automatically\n",
    "    overwrite the existing `.csv` file.  Otherwise it will just return the\n",
    "    combined data to the user as a pandas `DataFrame` for which they can\n",
    "    then save with the pandas `DataFrame.to_csv` method when desired.\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : str\n",
    "        The file path to the existing `.csv` file.\n",
    "    df : pandas DataFrame\n",
    "        A pandas `DataFrame` containing the new data collected.  It is\n",
    "        recommended that this `DataFrame` comes from the output of the\n",
    "        `to_pandas` method in `reddit_data_collector.io`.\n",
    "    key : str, default=\"id\"\n",
    "        The key to remove duplicate data on.  Default is the post or\n",
    "        comment `id` as set by Reddit.  It is not recommended to set\n",
    "        this parameter manually.  However, it is included as a parameter\n",
    "        in case for some reason duplicate data is desired.\n",
    "    sort : str, default=\"subreddit_name\"\n",
    "        How to sort the new data.  By default sorts the data by subreddit.\n",
    "        This is purely aesthetic and has no impact on the data itself.\n",
    "    save : bool, default=True\n",
    "        Whether or not to automatically overwrite the existing `.csv`\n",
    "        file with the new data.\n",
    "    Returns\n",
    "    -------\n",
    "    new_df : pandas DataFrame\n",
    "        A pandas `DataFrame` containing the newly combined post or comment\n",
    "        data.\n",
    "    Raises\n",
    "    ------\n",
    "    ColumnNameError\n",
    "        If the update is attempted with two pandas `DataFrame`s that have\n",
    "        different column names.\n",
    "    See Also\n",
    "    --------\n",
    "    reddit_data_collector.reddit_data_collector.DataCollector\n",
    "        Class that performs the data collection from Reddit.\n",
    "    reddit_data_collector.io.to_pandas\n",
    "        Used to convert raw `posts` or `comments` collected with\n",
    "        `DataCollector` to a pandas `DataFrame`.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import reddit_data_collector as rdc\n",
    "    >>> # create instance of DataCollector\n",
    "    >>> data_collector = rdc.DataCollector(\n",
    "    ...     \"<your_client_id>\",\n",
    "    ...     \"<your_client_secret>\",\n",
    "    ...     \"mac:script:v1.0 (by u/FakeRedditUser)\",\n",
    "    ...     \"FakeRedditUser\",\n",
    "    ...     \"FakePassword\"\n",
    "    ... )\n",
    "    >>> # collect some data from Reddit\n",
    "    >>> subreddits = [\"pics\", \"funny\"]\n",
    "    >>> post_filter = \"hot\"\n",
    "    >>> comment_data = True\n",
    "    >>> replies_data = True\n",
    "    >>> posts, comments = data_collector(\n",
    "    ...     subreddits=subreddits,\n",
    "    ...     post_filter=post_filter,\n",
    "    ...     comment_data=comment_data,\n",
    "    ...     replies_data=replies_data\n",
    "    ... )\n",
    "    >>> # update existing .csv file\n",
    "    >>> new_posts_df = rdc.update_data(\"post_data.csv\", posts_df)\n",
    "    >>> new_comments_df = rdc.update_data(\"comment_data.csv\", comments_df)\n",
    "    \"\"\"\n",
    "\n",
    "    if not set(pd.read_csv(csv_path).columns) == set(df.columns):\n",
    "        raise ColumnNameError(\"Both data sets must have the same features\")\n",
    "\n",
    "    old_df = pd.read_csv(csv_path)\n",
    "\n",
    "    new_df = (\n",
    "        pd.concat([old_df, df], ignore_index=True)\n",
    "        .drop_duplicates(subset=[key], ignore_index=True)\n",
    "        .sort_values(sort, ignore_index=True)\n",
    "    )\n",
    "\n",
    "    if save:\n",
    "        new_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    return new_df\n",
    "class SubredditError(Exception):\n",
    "    \"\"\"Exception class raised if invalid subreddit is used.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from reddit_data_collector.exceptions import SubredditError\n",
    "    >>> try:\n",
    "    ...     data_collector.get_data(subreddits=\"1nv4ald\")\n",
    "    ... except SubredditError as e:\n",
    "    ...     print(repr(e))\n",
    "    SubredditError('r/1nv4ald does not exist')\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class FilterError(Exception):\n",
    "    \"\"\"Exception class raised if an invalid post or top post filter is used.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from reddit_data_collector.exceptions import FilterError\n",
    "    >>> try:\n",
    "    ...     data_collector.get_data(subreddits=\"funny\", post_filter=\"any\")\n",
    "    ... except FilterError as e:\n",
    "    ...     print(repr(e))\n",
    "    FilterError('Invalid post_filter used: any')\n",
    "    >>> try:\n",
    "    ...     data_collector.get_data(\n",
    "    ...        subreddits=\"funny\",\n",
    "    ...        post_filter=\"top\",\n",
    "    ...        top_post_filter=\"now\"\n",
    "    ...     )\n",
    "    ... except FilterError as e:\n",
    "    ...     print(repr(e))\n",
    "    FilterError('Invalid top_post_filter used: now')\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class ColumnNameError(Exception):\n",
    "    \"\"\"Exception class used if data update is attempted with mismatched columns.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from reddit_data_collector.exceptions import ColumnNameError\n",
    "    >>> csv_path = \"example.csv\"\n",
    "    >>> # create and save first DataFrame\n",
    "    >>> df = pd.DataFrame(data=[[1, 2], [3, 4]], columns=[\"a\", \"b\"])\n",
    "    >>> df.to_csv(path, index=False)\n",
    "    >>> # create second DataFrame\n",
    "    >>> df2 = pd.DataFrame(data=[[5, 6], [7, 8]], columns=[\"c\", \"d\"])\n",
    "    >>> try:\n",
    "    ...    rdc.update_data(csv_path, df2)\n",
    "    ... except ColumnNameError as e:\n",
    "    ...    print(repr(e))\n",
    "    ColumnNameError('Both data sets must have the same features')\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "class DataCollector:\n",
    "    \"\"\"Object that performs data collection from Reddit.\n",
    "    Once a `DataCollector` object is instantiated, you simply need to pass the subreddit\n",
    "    name(s) that you desire to collect data from to the method `get_data`, and the data\n",
    "    collection will be performed.\n",
    "    Please see the Reddit's \"First Step Guide\" which describes how to obtain the\n",
    "    `client_id` and `client_secret` parameters below:\n",
    "    https://github.com/reddit-archive/reddit/wiki/OAuth2-Quick-Start-Example#first-steps\n",
    "    Important: If you instantiate `DataCollector` without a Reddit username and password,\n",
    "    it will have read only access to the reddit API, which is limited to 30 requests\n",
    "    per minute.  However, if you do provide a Reddit username and password, it will\n",
    "    have full access to the API and an increased limit of 60 requests per minute.  Full\n",
    "    access can increase data collection by 2x.\n",
    "    Finally, for safety, it is recommended that the parameters below are not hard-coded\n",
    "    directly into a program that uses Reddit Data Collector.  Rather, they should be\n",
    "    kept in a separate credentials file as data which is then read into the program.\n",
    "    (e.g. a JSON credentials file that is read into a program with a Python `with`\n",
    "    clause).\n",
    "    Parameters\n",
    "    ----------\n",
    "    client_id : str\n",
    "        The client id for your Reddit application.\n",
    "    client_secret : str\n",
    "        The client secret for your Reddit application.\n",
    "    user_agent : str\n",
    "        A unique identifier that helps Reddit determine the souce of network requests.\n",
    "        To use Reddit's API, you need a unique and descriptive user agent.  The\n",
    "        following format is recommended:\n",
    "            <platform>:<app ID>:<version string> (by u/<Reddit username>)\n",
    "    username : str, default=None\n",
    "        Your Reddit username.\n",
    "    password : str, default=None\n",
    "        Your Reddit password.\n",
    "    Attributes\n",
    "    ----------\n",
    "    reddit : praw.Reddit\n",
    "        An instance of the PRAW `Reddit` class that provides access to Reddit's API.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import reddit_data_collector as rdc\n",
    "    >>> # create instance of DataCollector\n",
    "    >>> data_collector = rdc.DataCollector(\n",
    "    ...     \"<your_client_id>\",\n",
    "    ...     \"<your_client_secret>\",\n",
    "    ...     \"mac:script:v1.0 (by u/FakeRedditUser)\",\n",
    "    ...     \"FakeRedditUser\",\n",
    "    ...     \"FakePassword\"\n",
    "    ... )\n",
    "    >>> # collect some data from Reddit\n",
    "    >>> posts, comments = data_collector.get_data(\n",
    "    ...     subreddits=[\"pics\", \"funny\"],\n",
    "    ...     post_filter=\"hot\",\n",
    "    ...     post_limit=10,\n",
    "    ...     comment_data=True,\n",
    "    ...     replies_data=True,\n",
    "    ...     replace_more_limit=0\n",
    "    ... )\n",
    "    >>> # save data as .csv files\n",
    "    >>> posts.to_csv(\"posts.csv\", index=False)\n",
    "    >>> comments.to_csv(\"posts.csv\", index=False)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, client_id, client_secret, user_agent, username=None, password=None\n",
    "    ):\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            user_agent=user_agent,\n",
    "            username=username,\n",
    "            password=password,\n",
    "        )\n",
    "\n",
    "    def get_data(\n",
    "        self,\n",
    "        subreddits,\n",
    "        post_filter=\"new\",\n",
    "        post_limit=None,\n",
    "        top_post_filter=None,\n",
    "        comment_data=True,\n",
    "        replies_data=False,\n",
    "        replace_more_limit=0,\n",
    "        dataframe=True,\n",
    "    ):\n",
    "        \"\"\"Collects post and comment data from Reddit.\n",
    "        Parameters\n",
    "        ----------\n",
    "        subreddits : str or list of str\n",
    "            The subreddit(s) to collect post and comment data from.\n",
    "        post_filter : str, default=\"new\"\n",
    "            How to filter the subreddit posts.  Must be one of:  new, hot, or top.\n",
    "        post_limit : int, default=None\n",
    "            The number of posts to collect.  A limit of `None` sets the limit to\n",
    "            the max allowed by Reddit's API, which is 1,000 in most cases.\n",
    "        top_post_filter : str, default=None\n",
    "            Determines how to filter the top posts for a subreddit.  Only required\n",
    "            if `post_filter` is set to \"top\". Must be one of: all, day, hour, month,\n",
    "            week, or year.\n",
    "        comment_data : bool, default=True\n",
    "            Whether or not to collect comment data for each post that is collected.\n",
    "            If only post data is desired, set to `False`.  Only collecting posts can\n",
    "            significantly speed up data collection since it will likely reduce the\n",
    "            number of API requests by a lot.\n",
    "        replies_data : bool, default=False\n",
    "            Whether or not to collect the data for all replies to each comment that\n",
    "            is collected for each post.  Setting this to `True` can cause the script\n",
    "            to take arbitrarily long, as some reddit comments can have arbitrarily\n",
    "            long reply threads.  Think carefully if you actually need this data before\n",
    "            setting this parameter to True.  Often times, reply threads will contain\n",
    "            useless data, since they often contain discussions of people trolling one\n",
    "            another.\n",
    "        replace_more_limit : int, default=0\n",
    "            The number of PRAW `MoreComment` instances to replace when collecting\n",
    "            comment data.  If you don't know what this means, the recommended\n",
    "            setting is a value between 0 to 32.  A higher value means that\n",
    "            potentially more comments will be collected in a sample. You can also\n",
    "            set this to `None` which will ensure all comments and replies on a\n",
    "            single post are collected.  Note that, setting this to an integer value\n",
    "            higher than 32 or to `None` can significantly slow down the script,\n",
    "            since this can increase the number of API calls drastically.\n",
    "            For more info on the PRAW `MoreComment` class read this:\n",
    "            https://praw.readthedocs.io/en/stable/tutorials/comments.html\n",
    "        dataframe : bool, default=True\n",
    "            Whether or not to return the collected data as a pandas DataFrame.\n",
    "            If False, the data is returned in the raw form of a dictionary,\n",
    "            where the keys for each dictionary are the subreddit name(s) and\n",
    "            the values for each dictionary are the data collected.\n",
    "        Returns\n",
    "        -------\n",
    "        posts, comments : pandas DataFrames\n",
    "            The collected reddit data.\n",
    "            If `comment_data` is False, `None` is returned for `comments`.\n",
    "        See Also\n",
    "        --------\n",
    "        reddit_data_collector.io.to_pandas\n",
    "            Used to convert raw `posts` or `comments` to a pandas `DataFrame`.\n",
    "            Not needed if dataframe argument is left as True.\n",
    "        reddit_data_collector.io.update_data\n",
    "            Used to update an existing `.csv` file that contains prior data collected\n",
    "            with Reddit Data Collector with new data collected.\n",
    "        \"\"\"\n",
    "        if isinstance(subreddits, str):\n",
    "            subreddits = [subreddits]\n",
    "\n",
    "        self._verify_subreddits(subreddits)\n",
    "        self._verify_post_filter(post_filter)\n",
    "\n",
    "        if top_post_filter is not None:\n",
    "            self._verify_top_post_filter(top_post_filter)\n",
    "\n",
    "        posts = self._get_posts(subreddits, post_filter, post_limit, top_post_filter)\n",
    "\n",
    "        if comment_data:\n",
    "            comments = self._get_comments(posts, replies_data, replace_more_limit)\n",
    "        else:\n",
    "            comments = None\n",
    "\n",
    "        if dataframe:\n",
    "            posts = to_pandas(posts)\n",
    "\n",
    "            if comments is not None:\n",
    "                comments = to_pandas(comments)\n",
    "\n",
    "        return posts, comments\n",
    "\n",
    "    # ------------------------------HELPER FUNCTIONS------------------------------ #\n",
    "\n",
    "    def _verify_subreddits(self, subreddits):\n",
    "        \"\"\"Verifies that each subreddit in a list of subreddits exist.\"\"\"\n",
    "        for subreddit in subreddits:\n",
    "            if not self._check_subreddit_exists(subreddit):\n",
    "                msg = f\"r/{subreddit} does not exist\"\n",
    "                raise (SubredditError(msg))\n",
    "\n",
    "    def _check_subreddit_exists(self, subreddit):\n",
    "        \"\"\"Checks if a subreddit exists.\"\"\"\n",
    "        # PRAW Subreddits instance\n",
    "        subreddits = self.reddit.subreddits\n",
    "\n",
    "        # may return numerous similar subreddits, first value should match\n",
    "        exists = subreddits.search_by_name(subreddit)\n",
    "\n",
    "        if not exists:\n",
    "            return False\n",
    "        else:\n",
    "            return exists[0].display_name.lower() == subreddit.lower()\n",
    "\n",
    "    def _verify_post_filter(self, post_filter):\n",
    "        \"\"\"Verifies that a post filter is valid.\n",
    "        Raises FilterError if a invalid post filter is used.\n",
    "        \"\"\"\n",
    "        if post_filter.lower() not in [\"new\", \"hot\", \"top\"]:\n",
    "            msg = f\"Invalid post_filter used: {post_filter}\"\n",
    "            raise (FilterError(msg))\n",
    "\n",
    "    def _verify_top_post_filter(self, top_post_filter):\n",
    "        \"\"\"Verifies that a top post filter is valid.\n",
    "        Raises FilterError if a invalid top post filter is used.\n",
    "        \"\"\"\n",
    "        if top_post_filter.lower() not in [\n",
    "            None,\n",
    "            \"all\",\n",
    "            \"day\",\n",
    "            \"hour\",\n",
    "            \"month\",\n",
    "            \"week\",\n",
    "            \"year\",\n",
    "        ]:\n",
    "            msg = f\"Invalid top_post_filter used: {top_post_filter}\"\n",
    "            raise (FilterError(msg))\n",
    "\n",
    "    def _get_posts(self, subreddits, post_filter, post_limit, top_post_filter):\n",
    "        \"\"\"Collects the post data for each subreddit in a list of subreddits.\"\"\"\n",
    "        posts = dict()\n",
    "\n",
    "        for subreddit in subreddits:\n",
    "            posts[subreddit] = self._get_subreddit_posts(\n",
    "                subreddit, post_filter, post_limit, top_post_filter\n",
    "            )\n",
    "\n",
    "        return posts\n",
    "\n",
    "    def _get_subreddit_posts(self, subreddit, post_filter, post_limit, top_post_filter):\n",
    "        \"\"\"Collects the post data for a single subreddit.\"\"\"\n",
    "        subreddit_posts = []\n",
    "\n",
    "        # convert to PRAW Subreddit instance\n",
    "        subreddit = self.reddit.subreddit(subreddit)\n",
    "\n",
    "        desc = f\"Collecting {post_filter} r/{subreddit} posts\"\n",
    "\n",
    "        # a \"submission\" is an instance of the PRAW Subission class\n",
    "        if post_filter.lower() == \"new\":\n",
    "            for submission in tqdm(subreddit.new(limit=post_limit), desc, post_limit):\n",
    "                subreddit_posts.append(self._get_post_data(submission))\n",
    "\n",
    "        elif post_filter.lower() == \"hot\":\n",
    "            for submission in tqdm(subreddit.hot(limit=post_limit), desc, post_limit):\n",
    "                subreddit_posts.append(self._get_post_data(submission))\n",
    "\n",
    "        elif post_filter.lower() == \"top\":\n",
    "            for submission in tqdm(subreddit.top(time_filter=top_post_filter), desc):\n",
    "                subreddit_posts.append(self._get_post_data(submission))\n",
    "\n",
    "        return subreddit_posts\n",
    "\n",
    "    def _get_post_data(self, submission):\n",
    "        \"\"\"Collects the data for a single post in a subreddit.\"\"\"\n",
    "        post_data = {\n",
    "            \"subreddit_name\": submission.subreddit.display_name,\n",
    "            \"post_created_utc\": submission.created_utc,\n",
    "            \"id\": submission.id,\n",
    "            \"selftext\": submission.selftext,\n",
    "            \"is_original_content\": submission.is_original_content,\n",
    "            \"is_self\": submission.is_self,\n",
    "            \"link_flair_text\": submission.link_flair_text,\n",
    "            \"locked\": submission.locked,\n",
    "            \"num_comments\": submission.num_comments,\n",
    "            \"over_18\": submission.over_18,\n",
    "            \"score\": submission.score,\n",
    "            \"spoiler\": submission.spoiler,\n",
    "            \"stickied\": submission.stickied,\n",
    "            \"title\": submission.title,\n",
    "            \"upvote_ratio\": submission.upvote_ratio,\n",
    "            \"url\": submission.url,\n",
    "        }\n",
    "\n",
    "        return post_data\n",
    "\n",
    "    def _get_comments(self, posts, replies_data, replace_more_limit):\n",
    "        \"\"\"Collects the comment data for each subreddit in a list of subreddits.\"\"\"\n",
    "        comments = dict()\n",
    "\n",
    "        for subreddit, subreddit_post_data in posts.items():\n",
    "            comments[subreddit] = self._get_subreddit_comments(\n",
    "                subreddit, subreddit_post_data, replies_data, replace_more_limit\n",
    "            )\n",
    "\n",
    "        return comments\n",
    "\n",
    "    def _get_subreddit_comments(\n",
    "        self, subreddit, subreddit_post_data, replies_data, replace_more_limit\n",
    "    ):\n",
    "        \"\"\"Collects the comment data for posts in a single subreddit.\"\"\"\n",
    "        subreddit_comments = []\n",
    "\n",
    "        desc = f\"Collecting comments for {len(subreddit_post_data)} r/{subreddit} posts\"\n",
    "\n",
    "        # a \"submission\" is an instance of the PRAW Subission class\n",
    "        for post in tqdm(subreddit_post_data, desc, len(subreddit_post_data)):\n",
    "            submission = self.reddit.submission(id=post[\"id\"])\n",
    "            submission.comments.replace_more(limit=replace_more_limit)\n",
    "\n",
    "            if replies_data:\n",
    "                for comment in submission.comments.list():\n",
    "                    comment_data = self._get_comment_data(subreddit, comment)\n",
    "                    subreddit_comments.append(comment_data)\n",
    "            else:\n",
    "                for comment in submission.comments:\n",
    "                    comment_data = self._get_comment_data(subreddit, comment)\n",
    "                    subreddit_comments.append(comment_data)\n",
    "\n",
    "        return subreddit_comments\n",
    "\n",
    "    def _get_comment_data(self, subreddit, comment):\n",
    "        \"\"\"Collects the data for a single comment on a subreddit post.\"\"\"\n",
    "        comment_data = {\n",
    "            \"subreddit_name\": subreddit,\n",
    "            \"id\": comment.id,\n",
    "            \"post_id\": comment.link_id,\n",
    "            \"parent_id\": comment.parent_id,\n",
    "            \"top_level_comment\": comment.parent_id == comment.link_id,\n",
    "            \"body\": comment.body,\n",
    "            \"comment_created_utc\": comment.created_utc,\n",
    "            \"is_submitter\": comment.is_submitter,\n",
    "            \"score\": comment.score,\n",
    "            \"stickied\": comment.stickied,\n",
    "        }\n",
    "\n",
    "        return comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "a_Tm_e9uInOT"
   },
   "outputs": [],
   "source": [
    "#import reddit_data_collector as rdc\n",
    "data_collector = DataCollector(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent,\n",
    "    username=username,\n",
    "    password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16833,
     "status": "ok",
     "timestamp": 1674623419598,
     "user": {
      "displayName": "Sunny Yang",
      "userId": "14334615308793811067"
     },
     "user_tz": 300
    },
    "id": "2TwNZNCFIwDd",
    "outputId": "488ec49b-40aa-480e-ecfb-f7e1d07f5084"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting new r/wallstreetbets posts:  48%|▍| 963/2000 [00:13<00:14, 69.31it/s]\n"
     ]
    }
   ],
   "source": [
    "posts, comments = data_collector.get_data(\n",
    "    subreddits=[\"wallstreetbets\"],\n",
    "    post_filter=\"new\",\n",
    "    post_limit=2000,\n",
    "    comment_data=False,\n",
    "    replies_data=False,\n",
    "    replace_more_limit=1000,\n",
    "    dataframe=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1674623425629,
     "user": {
      "displayName": "Sunny Yang",
      "userId": "14334615308793811067"
     },
     "user_tz": 300
    },
    "id": "P0YEgs3LJO7E",
    "outputId": "ef230ad6-dbd4-4376-ac8a-85564e0c527d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 866 entries, 0 to 865\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   subreddit_name       866 non-null    object \n",
      " 1   post_created_utc     866 non-null    float64\n",
      " 2   id                   866 non-null    object \n",
      " 3   selftext             866 non-null    object \n",
      " 4   is_original_content  866 non-null    bool   \n",
      " 5   is_self              866 non-null    bool   \n",
      " 6   link_flair_text      863 non-null    object \n",
      " 7   locked               866 non-null    bool   \n",
      " 8   num_comments         866 non-null    int64  \n",
      " 9   over_18              866 non-null    bool   \n",
      " 10  score                866 non-null    int64  \n",
      " 11  spoiler              866 non-null    bool   \n",
      " 12  stickied             866 non-null    bool   \n",
      " 13  title                866 non-null    object \n",
      " 14  upvote_ratio         866 non-null    float64\n",
      " 15  url                  866 non-null    object \n",
      "dtypes: bool(6), float64(2), int64(2), object(6)\n",
      "memory usage: 72.9+ KB\n"
     ]
    }
   ],
   "source": [
    "posts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArXeAHIjJV5d"
   },
   "outputs": [],
   "source": [
    "posts.to_csv(\"wsbnewdata.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
