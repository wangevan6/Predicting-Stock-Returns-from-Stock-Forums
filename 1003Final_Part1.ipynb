{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8bc1fc-8faf-44f0-8ae7-2274a26b0301",
   "metadata": {},
   "source": [
    "# DS1003 Final Project: Data \n",
    "## Step1: Collecting from Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44dab421-b594-4fa9-b998-bc52939a3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import praw\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a10f52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: reddit-data-collector in /Users/water/anaconda3/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: pandas>=1.3.5 in /Users/water/anaconda3/lib/python3.11/site-packages (from reddit-data-collector) (2.1.4)\n",
      "Requirement already satisfied: praw>=7.5.0 in /Users/water/anaconda3/lib/python3.11/site-packages (from reddit-data-collector) (7.7.1)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in /Users/water/anaconda3/lib/python3.11/site-packages (from reddit-data-collector) (4.65.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /Users/water/anaconda3/lib/python3.11/site-packages (from pandas>=1.3.5->reddit-data-collector) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/water/anaconda3/lib/python3.11/site-packages (from pandas>=1.3.5->reddit-data-collector) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/water/anaconda3/lib/python3.11/site-packages (from pandas>=1.3.5->reddit-data-collector) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/water/anaconda3/lib/python3.11/site-packages (from pandas>=1.3.5->reddit-data-collector) (2023.3)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /Users/water/anaconda3/lib/python3.11/site-packages (from praw>=7.5.0->reddit-data-collector) (2.4.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in /Users/water/anaconda3/lib/python3.11/site-packages (from praw>=7.5.0->reddit-data-collector) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Users/water/anaconda3/lib/python3.11/site-packages (from praw>=7.5.0->reddit-data-collector) (0.58.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /Users/water/anaconda3/lib/python3.11/site-packages (from prawcore<3,>=2.1->praw>=7.5.0->reddit-data-collector) (2.31.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/water/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->reddit-data-collector) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/water/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw>=7.5.0->reddit-data-collector) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/water/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw>=7.5.0->reddit-data-collector) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/water/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw>=7.5.0->reddit-data-collector) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/water/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw>=7.5.0->reddit-data-collector) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install reddit-data-collector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db7d89-f93b-47b3-b6b4-2daa4cb2e6a9",
   "metadata": {},
   "source": [
    "## Authenticating with the Reddit API\n",
    "\n",
    "This code snippet demonstrates how to authenticate and interact with the Reddit API using OAuth.\n",
    "\n",
    "### Key Components of the Code:\n",
    "1. **Authentication Setup**: \n",
    "   - Utilizes the `requests` library to manage HTTP requests.\n",
    "   - The credentials, including `CLIENT_ID` (personal use script ID) and `SECRET_TOKEN` (OAuth token), are specified to perform HTTP basic authentication.\n",
    "\n",
    "2. **Requesting an OAuth Token**:\n",
    "   - The script sends a POST request containing the user credentials (username and password) to obtain an OAuth access token.\n",
    "   - This token is essential for authenticating subsequent API requests.\n",
    "\n",
    "3. **Setting Headers**:\n",
    "   - A `User-Agent` header provides Reddit with a brief description of the bot application.\n",
    "   - After obtaining the OAuth access token, it's added to the headers to enable authorization for further API requests.\n",
    "\n",
    "4. **Making Authenticated Requests**:\n",
    "   - Once the OAuth token is acquired, the `Authorization` header is included in all subsequent requests.\n",
    "   - In this example, an authenticated request fetches the profile information of the currently authenticated user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f71189e-b48a-413c-988c-4e03a07019c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "auth = requests.auth.HTTPBasicAuth('3gn07Oqp2_sut_zD3n8Dnw', 'WED8ZTRAfziqJPu-uugMfFGVhl-ozw')\n",
    "\n",
    "# here we pass our login method (password), username, and password\n",
    "data = {'grant_type': 'password',\n",
    "        'username': 'diamonddurfhands',\n",
    "        'password': '**DiamondHands22'}\n",
    "\n",
    "# setup our header info, which gives reddit a brief description of our app\n",
    "headers = {'User-Agent': 'MyBot/0.0.1'}\n",
    "\n",
    "# send our request for an OAuth token\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                    auth=auth, data=data, headers=headers)\n",
    "\n",
    "# convert response to JSON and pull access_token value\n",
    "TOKEN = res.json()['access_token']\n",
    "\n",
    "# add authorization to our headers dictionary\n",
    "headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "\n",
    "# while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b487f1b-2a23-4a02-9228-a20fc4ff0a5b",
   "metadata": {},
   "source": [
    "## Using Pushshift API to Retrieve Reddit Submissions\n",
    "\n",
    "In this section, we access Reddit submissions via the Pushshift API. The collected data is organized into a pandas DataFrame for further analysis.\n",
    "\n",
    "### Key Components of the Code:\n",
    "1. **Pushshift API Request Function** (`getPushshiftData`):\n",
    "   - This function constructs a query URL based on parameters like title (`query`), time frame (`after`, `before`), and subreddit name (`sub`).\n",
    "   - The request is sent using the `requests` library, and the JSON response is parsed to obtain the submissions' data.\n",
    "\n",
    "2. **Data Extraction Function** (`collectSubData`):\n",
    "   - The function extracts key data points (title, URL, author, score, etc.) from each submission.\n",
    "   - Error handling ensures that missing flair and selftext fields are marked as \"NaN.\"\n",
    "   - The extracted data is organized into a pandas DataFrame.\n",
    "\n",
    "3. **Concatenating Submissions**:\n",
    "   - The submission data for each post is merged into a global DataFrame (`subStats_df`) using `pd.concat`.\n",
    "   - The global DataFrame is updated within the function to accumulate all the submissions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be4c0e2c-2db1-4761-99ee-646426fa2024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPushshiftData(query, after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch data, HTTP Status Code:\", response.status_code)\n",
    "        return pd.DataFrame() \n",
    "    if 'data' not in data or not data['data']:\n",
    "        print(\"No data returned from Pushshift.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if there's no data\n",
    "    data = json.loads(r.text)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "#This function will be used to extract the key data points from each JSON result\n",
    "def collectSubData(subm):\n",
    "    try:\n",
    "        flair = subm['link_flair_richtext']\n",
    "    except KeyError:\n",
    "        flair = \"NaN\"  # Handle missing flair\n",
    "    try:\n",
    "        selftext = subm['selftext']\n",
    "    except KeyError:\n",
    "        selftext = \"NaN\"  # Handle missing selftext\n",
    "\n",
    "    # Create a DataFrame for the single submission\n",
    "    subData = pd.DataFrame([{\n",
    "        'sub_id': subm['id'],\n",
    "        'title': subm['title'],\n",
    "        'url': subm['url'],\n",
    "        'author': subm['author'],\n",
    "        'score': subm['score'],\n",
    "        'created': datetime.datetime.fromtimestamp(subm['created_utc']),\n",
    "        'selftext': selftext,\n",
    "        'flair': flair\n",
    "    }])\n",
    "    # Use pd.concat to merge with the global DataFrame\n",
    "    global subStats_df  # To modify the global DataFrame within the function\n",
    "    subStats_df = pd.concat([subStats_df, new_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e45c82fd-a0f8-43e9-8feb-90bc04b81b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?title=DD&size=1000&after=1714521600&before=1715126399&subreddit=wallstreetbets\n",
      "Failed to fetch data, HTTP Status Code: 403\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Temp func: \n",
    "def getPushshiftData_2(query, after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title=' + str(query) + '&size=1000&after=' + str(after) + '&before=' + str(before) + '&subreddit=' + str(sub)\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch data, HTTP Status Code:\", response.status_code)\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on failure\n",
    "    data = json.loads(response.text)\n",
    "    if 'data' not in data or not data['data']:\n",
    "        print(\"No data returned from Pushshift.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if there's no data\n",
    "    return pd.DataFrame(data['data'])\n",
    "\n",
    "# Example usage\n",
    "query = \"DD\"\n",
    "after = \"1714521600\"  # Example start timestamp\n",
    "before = \"1715126399\"  # Example end timestamp\n",
    "sub = \"wallstreetbets\"\n",
    "\n",
    "data = getPushshiftData_2(query, after, before, sub)\n",
    "print(data.head())  # Print first few rows to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88323cc-0c39-4843-8913-608436049771",
   "metadata": {},
   "source": [
    "## Setting Up Search Parameters for the Pushshift API\n",
    "\n",
    "In this section, the code sets up essential search parameters for querying submissions from the Pushshift API.\n",
    "\n",
    "### Key Components of the Code:\n",
    "1. **Time Range**:\n",
    "   - The `after` and `before` variables represent Unix timestamps that define the time range for the search.\n",
    "   - Unix timestamps can be created or converted using online tools like [unixtimestamp.com](https://www.unixtimestamp.com/index.php).\n",
    "\n",
    "2. **Search Query**:\n",
    "   - The `query` variable specifies keywords to search for within submissions (e.g., \"DD\" for Deep Dive).\n",
    "\n",
    "3. **Subreddit**:\n",
    "   - The `sub` variable defines the specific subreddit to query, in this case, `wallstreetbets`.\n",
    "\n",
    "4. **Tracking Data Collection**:\n",
    "   - `subCount`: Tracks the total number of submissions collected.\n",
    "   - `subStats`: A dictionary used to store collected submissions data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e902909b-3d58-4b98-8c12-276e3442af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your timestamps and queries for your search URL\n",
    "#https://www.unixtimestamp.com/index.php > Use this to create your timestamps\n",
    "# we collected all the post from April 29th, 2024 -> May 3rd, 2024 \n",
    "\n",
    "after = \"1714521600\" #Submissions after this timestamp\n",
    "before = \"1715126399\" #Submissions before this timestamp\n",
    "query = \"DD\" #Keyword(s) to look for in submissions\n",
    "sub = \"wallstreetbets\" #Which Subreddit to search in\n",
    "\n",
    "#subCount tracks the no. of total submissions we collect\n",
    "subCount = 0\n",
    "#subStats is the dictionary where we will store our data.\n",
    "subStats = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a2a8d-ca69-4700-8cb9-7416128af275",
   "metadata": {},
   "source": [
    "## Iteratively Collecting Reddit Submissions\n",
    "\n",
    "In this section, we utilize the functions defined earlier to iteratively collect submissions using the Pushshift API, while adjusting the search range dynamically.\n",
    "\n",
    "### Key Components of the Code:\n",
    "1. **Initial API Call**:\n",
    "   - The function `getPushshiftData` is called first with the given parameters (`after`, `before`, `query`, and `sub`) to initialize the `data` variable.\n",
    "\n",
    "2. **While Loop for Data Collection**:\n",
    "   - The loop continues until `data` is empty, indicating no more submissions in the specified range.\n",
    "   - Inside the loop:\n",
    "     - **Data Extraction**: Each submission in the `data` list is passed to `collectSubData` for processing and storage, and the submission count (`subCount`) is incremented.\n",
    "     - **Updating the 'After' Variable**: The `after` variable is updated to the `created_utc` timestamp of the last submission retrieved. This ensures that the next API call continues where the previous left off.\n",
    "\n",
    "3. **Fetching More Data**:\n",
    "   - The API call is repeated using the updated `after` value, collecting more data within the specified range until no new data is found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbcdac95-a30f-40e5-a5e9-7ee0e7b601c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?title=DD&size=1000&after=1714521600&before=1715126399&subreddit=wallstreetbets\n",
      "Failed to fetch data, HTTP Status Code: 403\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# We need to run this function outside the loop first to get the updated after variable\n",
    "data = getPushshiftData(query, after, before, sub)\n",
    "# Will run until all posts have been gathered i.e. When the length of data variable = 0\n",
    "# from the 'after' date up until before date\n",
    "while len(data) > 0: #The length of data is the number submissions (data[0], data[1] etc), once it hits zero (after and before vars are the same) end\n",
    "    for submission in data:\n",
    "        collectSubData(submission)\n",
    "        subCount+=1\n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    print(len(data))\n",
    "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    #update after variable to last created date of submission\n",
    "    after = data[-1]['created_utc']\n",
    "    #data has changed due to the new after variable provided by above code\n",
    "    data = getPushshiftData(query, after, before, sub)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e605f3e7-a13e-4616-8563-f58ba6bd0144",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(subStats)) + \" submissions have added to list\")\n",
    "print(\"1st entry is:\")\n",
    "print(list(subStats.values())[0][0][1] + \" created: \" + str(list(subStats.values())[0][0][5]))\n",
    "print(\"Last entry is:\")\n",
    "print(list(subStats.values())[-1][0][1] + \" created: \" + str(list(subStats.values())[-1][0][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e38a1-9567-4968-ae80-e3e92b19265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateSubs_file():\n",
    "    upload_count = 0\n",
    "    #location = \"\\\\Reddit Data\\\\\" >> If you're running this outside of a notebook you'll need this to direct to a specific location\n",
    "    print(\"wsbpushdata.csv\")\n",
    "    filename = input() #This asks the user what to name the file\n",
    "    file = filename\n",
    "    with open(file, 'w', newline='', encoding='utf-8') as file:\n",
    "        a = csv.writer(file, delimiter=',')\n",
    "        headers = [\"Post ID\",\"Title\",\"Url\",\"Author\",\"Score\",\"Publish Date\", \"Selftext\", \"Flair\"]\n",
    "        a.writerow(headers)\n",
    "        for sub in subStats:\n",
    "            a.writerow(subStats[sub][0])\n",
    "            upload_count+=1\n",
    "\n",
    "        print(str(upload_count) + \" submissions have been uploaded\")\n",
    "updateSubs_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5111898-40b1-4a27-b738-efbd7024c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.to_csv(\"wsbnewdata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee897b4e-4e4a-44ef-832a-27b060b2f83c",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89306036-8a4c-4b25-8f09-69ebce22c7cf",
   "metadata": {},
   "source": [
    "## Setting Up Reddit API Credentials\n",
    "\n",
    "Here, we store sensitive Reddit API credentials and user details as variables to authenticate requests to Reddit's API.\n",
    "\n",
    "### Components Explained:\n",
    "\n",
    "1. **Credentials**:\n",
    "   - `client_id`: Unique identifier assigned to the application via Reddit's developer dashboard.\n",
    "   - `client_secret`: Secret token used alongside `client_id` for secure API access.\n",
    "   - `user_agent`: Custom identifier string that describes the application. This should include a name and version, like `\"myapp/0.1\"`.\n",
    "\n",
    "2. **User Credentials**:\n",
    "   - `username`: Reddit account username, necessary for authenticated access to private data or posting.\n",
    "   - `password`: Corresponding password to authenticate the Reddit account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a5d54-b285-4fb9-bc6c-0cd1b8250ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "client_id = \"3gn07Oqp2_sut_zD3n8Dnw\"\n",
    "client_secret = \"WED8ZTRAfziqJPu-uugMfFGVhl-ozw\"\n",
    "user_agent = \"ddhandsbot\"\n",
    "username = \"diamonddurfhands\"\n",
    "password = \"**DiamondHands22\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83732e3c-00f1-4c30-b808-7a1956cd25e5",
   "metadata": {},
   "source": [
    "### Overview of Reddit Data Collection\n",
    "\n",
    "This section describes the data collection process using a Reddit Data Collector class. The purpose is to gather structured data about posts and comments from specified subreddits on Reddit.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **`DataCollector` Class**  \n",
    "   - **Initialization**: Requires Reddit API credentials (client ID, secret, and user agent) and optionally the username and password to access the Reddit API.\n",
    "   - **Subreddit Verification**: The class checks if specified subreddits exist before attempting to collect data, raising a `SubredditError` if not.\n",
    "   - **Post Filters**: Validates the filtering method for posts (e.g., \"hot\", \"new\", \"top\") and raises a `FilterError` if an invalid filter is used.\n",
    "   - **Top Post Filters**: Similar validation ensures the \"top\" filter only uses valid parameters (like \"day,\" \"week,\" etc.).\n",
    "\n",
    "2. **Data Collection Methods**  \n",
    "   - **Post Data**: Retrieves posts based on the specified filter method (e.g., \"new,\" \"hot,\" \"top\"). Each post's attributes include title, score, URL, and other metadata.\n",
    "   - **Comment Data**: Optionally gathers comments and replies to each post if enabled. The depth of data collected can be configured via parameters.\n",
    "   - **Output Format**: Data can be returned as either pandas DataFrames or Python dictionaries for posts and comments.\n",
    "\n",
    "3. **Helper Functions**  \n",
    "   - Subreddit verification and filter validation functions assist with error handling.\n",
    "   - Post and comment retrieval functions are modular, streamlining data collection based on various criteria.\n",
    "\n",
    "The class ensures that users can efficiently collect data from Reddit by handling errors gracefully, maximizing API usage, and providing flexibility in data organization and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e5e40ad-c26e-4ea7-9d70-134956a95d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "def to_pandas(subreddit_data, separate=False):\n",
    "    \"\"\"Convert raw post or comment data collected to a pandas `DataFrame`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    subreddit_data : dict\n",
    "        Raw post or comment data collected with the `DataCollector.get_data`\n",
    "        method.\n",
    "    separate : bool, default=False\n",
    "        Whether or not to return a separate pandas `DataFrame` for the\n",
    "        data of each subreddit.\n",
    "    Returns\n",
    "    -------\n",
    "    df or dfs : pd.DataFrame or dict\n",
    "        If separate is `False`, returns a pandas `DataFrame` containing\n",
    "        the post or comment data.\n",
    "        If separate is `True` returns a Python dictionary containing\n",
    "        a pandas `DataFrame` for each subreddit that existed in the\n",
    "        post or comment data.  The dctionary keys are the subreddits\n",
    "        names.  The dictionary values are pandas `DataFrame`s of post\n",
    "        or comment data.\n",
    "    See Also\n",
    "    --------\n",
    "    reddit_data_collector.reddit_data_collector.DataCollector\n",
    "        Class that performs the data collection from Reddit.\n",
    "    reddit_data_collector.io.update_data\n",
    "        Update a `.csv` file containing existing post or comment\n",
    "        data collected with new data collected with `DataCollector`.\n",
    "    \"\"\"\n",
    "    dfs = dict()\n",
    "\n",
    "    for subreddit, data in subreddit_data.items():\n",
    "        dfs[subreddit] = pd.DataFrame(data)\n",
    "\n",
    "    if separate:\n",
    "        return dfs\n",
    "    else:\n",
    "        return pd.concat(dfs.values(), ignore_index=True)\n",
    "\n",
    "\n",
    "def update_data(csv_path, df, key=\"id\", sort=\"subreddit_name\", save=True):\n",
    "    \"\"\"Update a `.csv` file containing post or comment data with new data.\n",
    "    The main purpose of this method is to allow a user to update a `.csv`\n",
    "    file that contains historical data that they collected with Reddit Data\n",
    "    Collector with new data collected.  The default method settings ensure\n",
    "    that duplicated post or comment data, if any, is not saved to the `.csv`\n",
    "    file.  In other words only one copy of each post or comment is kept in\n",
    "    the combined data.\n",
    "    If the `save` parameter is set to `True` then the method will automatically\n",
    "    overwrite the existing `.csv` file.  Otherwise it will just return the\n",
    "    combined data to the user as a pandas `DataFrame` for which they can\n",
    "    then save with the pandas `DataFrame.to_csv` method when desired.\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : str\n",
    "        The file path to the existing `.csv` file.\n",
    "    df : pandas DataFrame\n",
    "        A pandas `DataFrame` containing the new data collected.  It is\n",
    "        recommended that this `DataFrame` comes from the output of the\n",
    "        `to_pandas` method in `reddit_data_collector.io`.\n",
    "    key : str, default=\"id\"\n",
    "        The key to remove duplicate data on.  Default is the post or\n",
    "        comment `id` as set by Reddit.  It is not recommended to set\n",
    "        this parameter manually.  However, it is included as a parameter\n",
    "        in case for some reason duplicate data is desired.\n",
    "    sort : str, default=\"subreddit_name\"\n",
    "        How to sort the new data.  By default sorts the data by subreddit.\n",
    "        This is purely aesthetic and has no impact on the data itself.\n",
    "    save : bool, default=True\n",
    "        Whether or not to automatically overwrite the existing `.csv`\n",
    "        file with the new data.\n",
    "    Returns\n",
    "    -------\n",
    "    new_df : pandas DataFrame\n",
    "        A pandas `DataFrame` containing the newly combined post or comment\n",
    "        data.\n",
    "    Raises\n",
    "    ------\n",
    "    ColumnNameError\n",
    "        If the update is attempted with two pandas `DataFrame`s that have\n",
    "        different column names.\n",
    "    See Also\n",
    "    --------\n",
    "    reddit_data_collector.reddit_data_collector.DataCollector\n",
    "        Class that performs the data collection from Reddit.\n",
    "    reddit_data_collector.io.to_pandas\n",
    "        Used to convert raw `posts` or `comments` collected with\n",
    "        `DataCollector` to a pandas `DataFrame`.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import reddit_data_collector as rdc\n",
    "    >>> # create instance of DataCollector\n",
    "    >>> data_collector = rdc.DataCollector(\n",
    "    ...     \"<your_client_id>\",\n",
    "    ...     \"<your_client_secret>\",\n",
    "    ...     \"mac:script:v1.0 (by u/FakeRedditUser)\",\n",
    "    ...     \"FakeRedditUser\",\n",
    "    ...     \"FakePassword\"\n",
    "    ... )\n",
    "    >>> # collect some data from Reddit\n",
    "    >>> subreddits = [\"pics\", \"funny\"]\n",
    "    >>> post_filter = \"hot\"\n",
    "    >>> comment_data = True\n",
    "    >>> replies_data = True\n",
    "    >>> posts, comments = data_collector(\n",
    "    ...     subreddits=subreddits,\n",
    "    ...     post_filter=post_filter,\n",
    "    ...     comment_data=comment_data,\n",
    "    ...     replies_data=replies_data\n",
    "    ... )\n",
    "    >>> # update existing .csv file\n",
    "    >>> new_posts_df = rdc.update_data(\"post_data.csv\", posts_df)\n",
    "    >>> new_comments_df = rdc.update_data(\"comment_data.csv\", comments_df)\n",
    "    \"\"\"\n",
    "\n",
    "    if not set(pd.read_csv(csv_path).columns) == set(df.columns):\n",
    "        raise ColumnNameError(\"Both data sets must have the same features\")\n",
    "\n",
    "    old_df = pd.read_csv(csv_path)\n",
    "\n",
    "    new_df = (\n",
    "        pd.concat([old_df, df], ignore_index=True)\n",
    "        .drop_duplicates(subset=[key], ignore_index=True)\n",
    "        .sort_values(sort, ignore_index=True)\n",
    "    )\n",
    "\n",
    "    if save:\n",
    "        new_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    return new_df\n",
    "class SubredditError(Exception):\n",
    "    \"\"\"Exception class raised if invalid subreddit is used.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from reddit_data_collector.exceptions import SubredditError\n",
    "    >>> try:\n",
    "    ...     data_collector.get_data(subreddits=\"1nv4ald\")\n",
    "    ... except SubredditError as e:\n",
    "    ...     print(repr(e))\n",
    "    SubredditError('r/1nv4ald does not exist')\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class FilterError(Exception):\n",
    "    \"\"\"Exception class raised if an invalid post or top post filter is used.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from reddit_data_collector.exceptions import FilterError\n",
    "    >>> try:\n",
    "    ...     data_collector.get_data(subreddits=\"funny\", post_filter=\"any\")\n",
    "    ... except FilterError as e:\n",
    "    ...     print(repr(e))\n",
    "    FilterError('Invalid post_filter used: any')\n",
    "    >>> try:\n",
    "    ...     data_collector.get_data(\n",
    "    ...        subreddits=\"funny\",\n",
    "    ...        post_filter=\"top\",\n",
    "    ...        top_post_filter=\"now\"\n",
    "    ...     )\n",
    "    ... except FilterError as e:\n",
    "    ...     print(repr(e))\n",
    "    FilterError('Invalid top_post_filter used: now')\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class ColumnNameError(Exception):\n",
    "    \"\"\"Exception class used if data update is attempted with mismatched columns.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from reddit_data_collector.exceptions import ColumnNameError\n",
    "    >>> csv_path = \"example.csv\"\n",
    "    >>> # create and save first DataFrame\n",
    "    >>> df = pd.DataFrame(data=[[1, 2], [3, 4]], columns=[\"a\", \"b\"])\n",
    "    >>> df.to_csv(path, index=False)\n",
    "    >>> # create second DataFrame\n",
    "    >>> df2 = pd.DataFrame(data=[[5, 6], [7, 8]], columns=[\"c\", \"d\"])\n",
    "    >>> try:\n",
    "    ...    rdc.update_data(csv_path, df2)\n",
    "    ... except ColumnNameError as e:\n",
    "    ...    print(repr(e))\n",
    "    ColumnNameError('Both data sets must have the same features')\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "class DataCollector:\n",
    "    \"\"\"Object that performs data collection from Reddit.\n",
    "    Once a `DataCollector` object is instantiated, you simply need to pass the subreddit\n",
    "    name(s) that you desire to collect data from to the method `get_data`, and the data\n",
    "    collection will be performed.\n",
    "    Please see the Reddit's \"First Step Guide\" which describes how to obtain the\n",
    "    `client_id` and `client_secret` parameters below:\n",
    "    https://github.com/reddit-archive/reddit/wiki/OAuth2-Quick-Start-Example#first-steps\n",
    "    Important: If you instantiate `DataCollector` without a Reddit username and password,\n",
    "    it will have read only access to the reddit API, which is limited to 30 requests\n",
    "    per minute.  However, if you do provide a Reddit username and password, it will\n",
    "    have full access to the API and an increased limit of 60 requests per minute.  Full\n",
    "    access can increase data collection by 2x.\n",
    "    Finally, for safety, it is recommended that the parameters below are not hard-coded\n",
    "    directly into a program that uses Reddit Data Collector.  Rather, they should be\n",
    "    kept in a separate credentials file as data which is then read into the program.\n",
    "    (e.g. a JSON credentials file that is read into a program with a Python `with`\n",
    "    clause).\n",
    "    Parameters\n",
    "    ----------\n",
    "    client_id : str\n",
    "        The client id for your Reddit application.\n",
    "    client_secret : str\n",
    "        The client secret for your Reddit application.\n",
    "    user_agent : str\n",
    "        A unique identifier that helps Reddit determine the souce of network requests.\n",
    "        To use Reddit's API, you need a unique and descriptive user agent.  The\n",
    "        following format is recommended:\n",
    "            <platform>:<app ID>:<version string> (by u/<Reddit username>)\n",
    "    username : str, default=None\n",
    "        Your Reddit username.\n",
    "    password : str, default=None\n",
    "        Your Reddit password.\n",
    "    Attributes\n",
    "    ----------\n",
    "    reddit : praw.Reddit\n",
    "        An instance of the PRAW `Reddit` class that provides access to Reddit's API.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import reddit_data_collector as rdc\n",
    "    >>> # create instance of DataCollector\n",
    "    >>> data_collector = rdc.DataCollector(\n",
    "    ...     \"<your_client_id>\",\n",
    "    ...     \"<your_client_secret>\",\n",
    "    ...     \"mac:script:v1.0 (by u/FakeRedditUser)\",\n",
    "    ...     \"FakeRedditUser\",\n",
    "    ...     \"FakePassword\"\n",
    "    ... )\n",
    "    >>> # collect some data from Reddit\n",
    "    >>> posts, comments = data_collector.get_data(\n",
    "    ...     subreddits=[\"pics\", \"funny\"],\n",
    "    ...     post_filter=\"hot\",\n",
    "    ...     post_limit=10,\n",
    "    ...     comment_data=True,\n",
    "    ...     replies_data=True,\n",
    "    ...     replace_more_limit=0\n",
    "    ... )\n",
    "    >>> # save data as .csv files\n",
    "    >>> posts.to_csv(\"posts.csv\", index=False)\n",
    "    >>> comments.to_csv(\"posts.csv\", index=False)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, client_id, client_secret, user_agent, username=None, password=None\n",
    "    ):\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            user_agent=user_agent,\n",
    "            username=username,\n",
    "            password=password,\n",
    "        )\n",
    "\n",
    "    def get_data(\n",
    "        self,\n",
    "        subreddits,\n",
    "        post_filter=\"new\",\n",
    "        post_limit=None,\n",
    "        top_post_filter=None,\n",
    "        comment_data=True,\n",
    "        replies_data=False,\n",
    "        replace_more_limit=0,\n",
    "        dataframe=True,\n",
    "    ):\n",
    "        \"\"\"Collects post and comment data from Reddit.\n",
    "        Parameters\n",
    "        ----------\n",
    "        subreddits : str or list of str\n",
    "            The subreddit(s) to collect post and comment data from.\n",
    "        post_filter : str, default=\"new\"\n",
    "            How to filter the subreddit posts.  Must be one of:  new, hot, or top.\n",
    "        post_limit : int, default=None\n",
    "            The number of posts to collect.  A limit of `None` sets the limit to\n",
    "            the max allowed by Reddit's API, which is 1,000 in most cases.\n",
    "        top_post_filter : str, default=None\n",
    "            Determines how to filter the top posts for a subreddit.  Only required\n",
    "            if `post_filter` is set to \"top\". Must be one of: all, day, hour, month,\n",
    "            week, or year.\n",
    "        comment_data : bool, default=True\n",
    "            Whether or not to collect comment data for each post that is collected.\n",
    "            If only post data is desired, set to `False`.  Only collecting posts can\n",
    "            significantly speed up data collection since it will likely reduce the\n",
    "            number of API requests by a lot.\n",
    "        replies_data : bool, default=False\n",
    "            Whether or not to collect the data for all replies to each comment that\n",
    "            is collected for each post.  Setting this to `True` can cause the script\n",
    "            to take arbitrarily long, as some reddit comments can have arbitrarily\n",
    "            long reply threads.  Think carefully if you actually need this data before\n",
    "            setting this parameter to True.  Often times, reply threads will contain\n",
    "            useless data, since they often contain discussions of people trolling one\n",
    "            another.\n",
    "        replace_more_limit : int, default=0\n",
    "            The number of PRAW `MoreComment` instances to replace when collecting\n",
    "            comment data.  If you don't know what this means, the recommended\n",
    "            setting is a value between 0 to 32.  A higher value means that\n",
    "            potentially more comments will be collected in a sample. You can also\n",
    "            set this to `None` which will ensure all comments and replies on a\n",
    "            single post are collected.  Note that, setting this to an integer value\n",
    "            higher than 32 or to `None` can significantly slow down the script,\n",
    "            since this can increase the number of API calls drastically.\n",
    "            For more info on the PRAW `MoreComment` class read this:\n",
    "            https://praw.readthedocs.io/en/stable/tutorials/comments.html\n",
    "        dataframe : bool, default=True\n",
    "            Whether or not to return the collected data as a pandas DataFrame.\n",
    "            If False, the data is returned in the raw form of a dictionary,\n",
    "            where the keys for each dictionary are the subreddit name(s) and\n",
    "            the values for each dictionary are the data collected.\n",
    "        Returns\n",
    "        -------\n",
    "        posts, comments : pandas DataFrames\n",
    "            The collected reddit data.\n",
    "            If `comment_data` is False, `None` is returned for `comments`.\n",
    "        See Also\n",
    "        --------\n",
    "        reddit_data_collector.io.to_pandas\n",
    "            Used to convert raw `posts` or `comments` to a pandas `DataFrame`.\n",
    "            Not needed if dataframe argument is left as True.\n",
    "        reddit_data_collector.io.update_data\n",
    "            Used to update an existing `.csv` file that contains prior data collected\n",
    "            with Reddit Data Collector with new data collected.\n",
    "        \"\"\"\n",
    "        if isinstance(subreddits, str):\n",
    "            subreddits = [subreddits]\n",
    "\n",
    "        self._verify_subreddits(subreddits)\n",
    "        self._verify_post_filter(post_filter)\n",
    "\n",
    "        if top_post_filter is not None:\n",
    "            self._verify_top_post_filter(top_post_filter)\n",
    "\n",
    "        posts = self._get_posts(subreddits, post_filter, post_limit, top_post_filter)\n",
    "\n",
    "        if comment_data:\n",
    "            comments = self._get_comments(posts, replies_data, replace_more_limit)\n",
    "        else:\n",
    "            comments = None\n",
    "\n",
    "        if dataframe:\n",
    "            posts = to_pandas(posts)\n",
    "\n",
    "            if comments is not None:\n",
    "                comments = to_pandas(comments)\n",
    "\n",
    "        return posts, comments\n",
    "\n",
    "\n",
    "    def get_data_2(\n",
    "        self,\n",
    "        subreddits,\n",
    "        post_filter=\"new\",\n",
    "        post_limit=None,\n",
    "        top_post_filter=None,\n",
    "        start_timestamp=None,\n",
    "        end_timestamp=None,\n",
    "        comment_data=True,\n",
    "        replies_data=False,\n",
    "        replace_more_limit=0,\n",
    "        dataframe=True,\n",
    "    ):\n",
    "        if isinstance(subreddits, str):\n",
    "            subreddits = [subreddits]\n",
    "    \n",
    "        self._verify_subreddits(subreddits)\n",
    "        self._verify_post_filter(post_filter)\n",
    "    \n",
    "        if top_post_filter is not None:\n",
    "            self._verify_top_post_filter(top_post_filter)\n",
    "    \n",
    "        posts = self._get_posts(subreddits, post_filter, post_limit, top_post_filter)\n",
    "    \n",
    "        # Filter posts by timestamp if both start and end timestamps are provided\n",
    "        if start_timestamp and end_timestamp:\n",
    "            for subreddit in posts:\n",
    "                posts[subreddit] = [post for post in posts[subreddit] if start_timestamp <= post[\"created_utc\"] <= end_timestamp]\n",
    "    \n",
    "        if comment_data:\n",
    "            comments = self._get_comments(posts, replies_data, replace_more_limit)\n",
    "        else:\n",
    "            comments = None\n",
    "    \n",
    "        if dataframe:\n",
    "            posts = to_pandas(posts)\n",
    "    \n",
    "            if comments is not None:\n",
    "                comments = to_pandas(comments)\n",
    "    \n",
    "        return posts, comments\n",
    "\n",
    "    # ------------------------------HELPER FUNCTIONS------------------------------ #\n",
    "\n",
    "    def _verify_subreddits(self, subreddits):\n",
    "        \"\"\"Verifies that each subreddit in a list of subreddits exist.\"\"\"\n",
    "        for subreddit in subreddits:\n",
    "            if not self._check_subreddit_exists(subreddit):\n",
    "                msg = f\"r/{subreddit} does not exist\"\n",
    "                raise (SubredditError(msg))\n",
    "\n",
    "    def _check_subreddit_exists(self, subreddit):\n",
    "        \"\"\"Checks if a subreddit exists.\"\"\"\n",
    "        # PRAW Subreddits instance\n",
    "        subreddits = self.reddit.subreddits\n",
    "\n",
    "        # may return numerous similar subreddits, first value should match\n",
    "        exists = subreddits.search_by_name(subreddit)\n",
    "\n",
    "        if not exists:\n",
    "            return False\n",
    "        else:\n",
    "            return exists[0].display_name.lower() == subreddit.lower()\n",
    "\n",
    "    def _verify_post_filter(self, post_filter):\n",
    "        \"\"\"Verifies that a post filter is valid.\n",
    "        Raises FilterError if a invalid post filter is used.\n",
    "        \"\"\"\n",
    "        if post_filter.lower() not in [\"new\", \"hot\", \"top\"]:\n",
    "            msg = f\"Invalid post_filter used: {post_filter}\"\n",
    "            raise (FilterError(msg))\n",
    "\n",
    "    def _verify_top_post_filter(self, top_post_filter):\n",
    "        \"\"\"Verifies that a top post filter is valid.\n",
    "        Raises FilterError if a invalid top post filter is used.\n",
    "        \"\"\"\n",
    "        if top_post_filter.lower() not in [\n",
    "            None,\n",
    "            \"all\",\n",
    "            \"day\",\n",
    "            \"hour\",\n",
    "            \"month\",\n",
    "            \"week\",\n",
    "            \"year\",\n",
    "        ]:\n",
    "            msg = f\"Invalid top_post_filter used: {top_post_filter}\"\n",
    "            raise (FilterError(msg))\n",
    "\n",
    "    def _get_posts(self, subreddits, post_filter, post_limit, top_post_filter):\n",
    "        \"\"\"Collects the post data for each subreddit in a list of subreddits.\"\"\"\n",
    "        posts = dict()\n",
    "\n",
    "        for subreddit in subreddits:\n",
    "            posts[subreddit] = self._get_subreddit_posts(\n",
    "                subreddit, post_filter, post_limit, top_post_filter\n",
    "            )\n",
    "\n",
    "        return posts\n",
    "\n",
    "    def _get_subreddit_posts(self, subreddit, post_filter, post_limit, top_post_filter):\n",
    "        \"\"\"Collects the post data for a single subreddit.\"\"\"\n",
    "        subreddit_posts = []\n",
    "\n",
    "        # convert to PRAW Subreddit instance\n",
    "        subreddit = self.reddit.subreddit(subreddit)\n",
    "\n",
    "        desc = f\"Collecting {post_filter} r/{subreddit} posts\"\n",
    "\n",
    "        # a \"submission\" is an instance of the PRAW Subission class\n",
    "        if post_filter.lower() == \"new\":\n",
    "            for submission in tqdm(subreddit.new(limit=post_limit), desc, post_limit):\n",
    "                subreddit_posts.append(self._get_post_data(submission))\n",
    "\n",
    "        elif post_filter.lower() == \"hot\":\n",
    "            for submission in tqdm(subreddit.hot(limit=post_limit), desc, post_limit):\n",
    "                subreddit_posts.append(self._get_post_data(submission))\n",
    "\n",
    "        elif post_filter.lower() == \"top\":\n",
    "            for submission in tqdm(subreddit.top(time_filter=top_post_filter), desc):\n",
    "                subreddit_posts.append(self._get_post_data(submission))\n",
    "\n",
    "        return subreddit_posts\n",
    "\n",
    "    def _get_post_data(self, submission):\n",
    "        \"\"\"Collects the data for a single post in a subreddit.\"\"\"\n",
    "        post_data = {\n",
    "            \"subreddit_name\": submission.subreddit.display_name,\n",
    "            \"post_created_utc\": submission.created_utc,\n",
    "            \"id\": submission.id,\n",
    "            \"selftext\": submission.selftext,\n",
    "            \"is_original_content\": submission.is_original_content,\n",
    "            \"is_self\": submission.is_self,\n",
    "            \"link_flair_text\": submission.link_flair_text,\n",
    "            \"locked\": submission.locked,\n",
    "            \"num_comments\": submission.num_comments,\n",
    "            \"over_18\": submission.over_18,\n",
    "            \"score\": submission.score,\n",
    "            \"spoiler\": submission.spoiler,\n",
    "            \"stickied\": submission.stickied,\n",
    "            \"title\": submission.title,\n",
    "            \"upvote_ratio\": submission.upvote_ratio,\n",
    "            \"url\": submission.url,\n",
    "        }\n",
    "\n",
    "        return post_data\n",
    "\n",
    "    def _get_comments(self, posts, replies_data, replace_more_limit):\n",
    "        \"\"\"Collects the comment data for each subreddit in a list of subreddits.\"\"\"\n",
    "        comments = dict()\n",
    "\n",
    "        for subreddit, subreddit_post_data in posts.items():\n",
    "            comments[subreddit] = self._get_subreddit_comments(\n",
    "                subreddit, subreddit_post_data, replies_data, replace_more_limit\n",
    "            )\n",
    "\n",
    "        return comments\n",
    "\n",
    "    def _get_subreddit_comments(\n",
    "        self, subreddit, subreddit_post_data, replies_data, replace_more_limit\n",
    "    ):\n",
    "        \"\"\"Collects the comment data for posts in a single subreddit.\"\"\"\n",
    "        subreddit_comments = []\n",
    "\n",
    "        desc = f\"Collecting comments for {len(subreddit_post_data)} r/{subreddit} posts\"\n",
    "\n",
    "        # a \"submission\" is an instance of the PRAW Subission class\n",
    "        for post in tqdm(subreddit_post_data, desc, len(subreddit_post_data)):\n",
    "            submission = self.reddit.submission(id=post[\"id\"])\n",
    "            submission.comments.replace_more(limit=replace_more_limit)\n",
    "\n",
    "            if replies_data:\n",
    "                for comment in submission.comments.list():\n",
    "                    comment_data = self._get_comment_data(subreddit, comment)\n",
    "                    subreddit_comments.append(comment_data)\n",
    "            else:\n",
    "                for comment in submission.comments:\n",
    "                    comment_data = self._get_comment_data(subreddit, comment)\n",
    "                    subreddit_comments.append(comment_data)\n",
    "\n",
    "        return subreddit_comments\n",
    "\n",
    "    def _get_comment_data(self, subreddit, comment):\n",
    "        \"\"\"Collects the data for a single comment on a subreddit post.\"\"\"\n",
    "        comment_data = {\n",
    "            \"subreddit_name\": subreddit,\n",
    "            \"id\": comment.id,\n",
    "            \"post_id\": comment.link_id,\n",
    "            \"parent_id\": comment.parent_id,\n",
    "            \"top_level_comment\": comment.parent_id == comment.link_id,\n",
    "            \"body\": comment.body,\n",
    "            \"comment_created_utc\": comment.created_utc,\n",
    "            \"is_submitter\": comment.is_submitter,\n",
    "            \"score\": comment.score,\n",
    "            \"stickied\": comment.stickied,\n",
    "        }\n",
    "\n",
    "        return comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b2c0fc6-e9c5-45eb-afb7-7dbb7cb03494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import reddit_data_collector as rdc\n",
    "data_collector = DataCollector(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent,\n",
    "    username=username,\n",
    "    password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15019b-a202-4d3e-bad1-341236828b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unix timestamps (replace these with your desired start and end times)\n",
    "after = 1714521600  # Example timestamp\n",
    "before = 1715126399  # Example timestamp\n",
    "\n",
    "# Retrieve posts within the specified timestamp range\n",
    "posts, comments = data_collector.get_data_2(\n",
    "    subreddits=[\"wallstreetbets\"],\n",
    "    post_filter=\"new\",\n",
    "    post_limit=2000,\n",
    "    start_timestamp=after,\n",
    "    end_timestamp=before,\n",
    "    comment_data=False,\n",
    "    replies_data=False,\n",
    "    replace_more_limit=1000,\n",
    "    dataframe=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
